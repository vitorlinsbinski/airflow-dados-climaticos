# ğŸŒ¤ï¸ Weather Data Pipeline with Apache Airflow

This project implements a weather data pipeline using Apache Airflow. The pipeline automates the extraction, transformation, and storage of weather data for the city of Boston, MA, on a weekly basis.

## âœ¨ Features

- **ğŸ“¥ Data Extraction**: Fetches weather data from the Visual Crossing Weather API for a 7-day period starting from the specified date.
- **ğŸ”„ Data Transformation**: Processes the raw data into three separate CSV files:
    - `ğŸ“„ dados_brutos.csv`: Contains the raw weather data.
    - `ğŸ“„ temperaturas.csv`: Includes minimum, current, and maximum temperatures for each day.
    - `ğŸ“„ condicoes.csv`: Contains weather descriptions and icons for each day.
- **ğŸ’¾ Data Storage**: Saves the processed files into a directory named after the week (`semana=YYYY-MM-DD`).

## ğŸ“‚ Project Structure

- **ğŸ—‚ï¸ DAGs**:
    - `ğŸ“œ dados_climaticos.py`: Defines the Airflow DAG for the weather data pipeline.
    - `ğŸ“œ atividade_aula_4_dag.py` and `ğŸ“œ meu_primeiro_dag.py`: Example DAGs for learning purposes.
- **ğŸ“ Data Directories**:
    - `ğŸ“‚ dags/semana=YYYY-MM-DD/`: Stores the generated CSV files for each week.
- **âš™ï¸ Configuration**:
    - `âš™ï¸ airflow.cfg`: Airflow configuration file.
    - `âš™ï¸ webserver_config.py`: Webserver configuration for Airflow.

## ğŸ› ï¸ How It Works

1. **ğŸ“‚ Folder Creation**: The pipeline creates a directory for the current week's data.
2. **ğŸŒ Data Fetching**: Weather data is fetched from the API using the `PythonOperator`.
3. **ğŸ§¹ Data Processing**: The raw data is split into three CSV files for easier analysis.
4. **â° Scheduling**: The DAG runs every Monday at midnight (`0 0 * * 1`).

## ğŸ“‹ Requirements

- **ğŸ Python Libraries**:
    - `pandas`
    - `pendulum`
    - `apache-airflow`
- **ğŸ”‘ API Key**: A valid API key for the Visual Crossing Weather API.

## ğŸš€ Usage

1. Clone the repository and set up an Airflow environment.
2. Place your API key in the `extrai_dados` function in `dados_climaticos.py`.
3. Start the Airflow scheduler and webserver.
4. Trigger the `dados_climaticos` DAG manually or let it run on schedule.

## ğŸ“Š Example Output

The pipeline generates the following files for each week:

- `ğŸ“„ dados_brutos.csv`: Raw weather data.
- `ğŸ“„ temperaturas.csv`: Daily temperature statistics.
- `ğŸ“„ condicoes.csv`: Weather conditions and descriptions.

## ğŸƒ How to Run

Follow the steps below to run the project locally with Apache Airflow:

### 1. ğŸ–¥ï¸ Clone the repository

```bash
git clone https://github.com/vitorlinsbinski/airflow-dados-climaticos.git
cd airflow-dados-climaticos
```

### 2. ğŸ Create and activate a virtual environment 
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
```

### 3. ğŸ“¦ Install dependencies
```bash
pip install pandas pendulum
pip install "apache-airflow==2.10.5" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.5/constraints-3.9.txt"
```

### 4. ğŸŒ Set the AIRFLOW_HOME environment variable
```bash
export AIRFLOW_HOME=$(pwd)  # Linux/macOS
```

### 5. ğŸ”‘ Insert your API Key
Open the file `dados_climaticos.py` and replace the placeholder with your Visual Crossing API key:
```py
key = 'YOUR-API-KEY'
```

### 6. âš™ï¸ Initialize Airflow
```bash
airflow standalone
```

### 7. Data Pipeline Screenshot
![Airflow Data Pipeline Screenshot](./assets/airflow-screenshot.png)